
import tensorflow as tf

class TRNNConfig (object):
    "" "RNN configuration parameter" ""

    # Model parameters
    embedding_dim = 64 # word vector dimension
    seq_length = 600 # sequence length
    num_classes = 10 # The number of categories
    vocab_size = 5000 # Vocabulary expression is small

    num_layers = 2 # Hidden layers
    hidden_dim = 128 # Hidden layer neurons
    rnn = 'gru' # lstm or gru

    dropout_keep_prob = 0.8 # dropout retention ratio
    learning_rate = 1e-3 # The learning rate

    batch_size = 128 # each training size
    num_epochs = 10 # total iteration rounds

    print_per_batch = 100 # The number of rounds to output a result
    save_per_batch = 10 # how many rounds into tensorboard


class TextRNN (object):
    "" "Text classification, RNN model" ""
    def __init __ (self, config):
        self.config = config

        # Three data to be entered
        self.input_x = tf.placeholder (tf.int32, [None, self.config.seq_length], name = 'input_x')
        self.input_y = tf.placeholder (tf.float32, [None, self.config.num_classes], name = 'input_y')
        self.keep_prob = tf.placeholder (tf.float32, name = 'keep_prob')

        self.rnn ()
        
    def rnn (self):
        "" rnn model "" "

        def lstm_cell (): # lstm core
            return tf.contrib.rnn.BasicLSTMCell (self.config.hidden_dim, state_is_tuple = True)

        def gru_cell (): # gru core
            return tf.contrib.rnn.GRUCell (self.config.hidden_dim)

        def dropout (): # Add a dropout layer to each rnn core
            if (self.config.rnn == 'lstm'):
                cell = lstm_cell ()
            else:
                cell = gru_cell ()
            return tf.contrib.rnn.DropoutWrapper (cell, output_keep_prob = self.keep_prob)

        # Word vector mapping
        with tf.device ('/ cpu: 0'):
            embedding = tf.get_variable ('embedding', [self.config.vocab_size, self.config.embedding_dim])
            embedding_inputs = tf.nn.embedding_lookup (embedding, self.input_x)

        with tf.name_scope ("rnn"):
            # Multi-layer rnn network
            cells = [dropout () for _ in range (self.config.num_layers)]
            rnn_cell = tf.contrib.rnn.MultiRNNCell (cells, state_is_tuple = True)

            _outputs, _ = tf.nn.dynamic_rnn (cell = rnn_cell, inputs = embedding_inputs, dtype = tf.float32)
            last = _outputs [:, -1,:] # Take the last timing output as the result
            with tf.name_scope ("score"):
            # All connected layer, followed by dropout and relu activation
            fc = tf.layers.dense (last, self.config.hidden_dim, name = 'fc1')
            fc = tf.contrib.layers.dropout (fc, self.keep_prob)
            fc = tf.nn.relu (fc)

            # Classifier
            self.logits = tf.layers.dense (fc, self.config.num_classes, name = 'fc2')
            self.y_pred_cls = tf.argmax (tf.nn.softmax (self.logits), 1) # forecast category

        with tf.name_scope ("optimize"):
            # Loss function, cross entropy
            cross_entropy = tf.nn.softmax_cross_entropy_with_logits (logits = self.logits, labels = self.input_y)
            self.loss = tf.reduce_mean (cross_entropy)
            # Optimizer
            self.optim = tf.train.AdamOptimizer (learning_rate = self.config.learning_rate) .minimize (self.loss)

        with tf.name_scope ("accuracy"):
            # Accuracy
            correct_pred = tf.equal (tf.argmax (self.input_y, 1), self.y_pred_cls)
            self.acc = tf.reduce_mean (tf.cast (correct_pred, tf.float32))
